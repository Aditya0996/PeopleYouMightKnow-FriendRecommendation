{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwg0lYX2o6Qg4tmKbM9GYt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya0996/PeopleYouMightKnow-FriendRecommendation/blob/main/%E2%80%9CPeople_You_Might_Know%E2%80%9D_social_network_friendship_recommendation_algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Import Data and Library"
      ],
      "metadata": {
        "id": "PKSkEMSpBixa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKIN7t8D9m_H",
        "outputId": "e54dadac-5f21-4ad2-fc34-aed4a180a3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u382-ga-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 18 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clFplK6G8y1s"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# create the Spark Session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# create the Spark Context\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "conf=SparkConf().set(\"spark.executor.memory\", \"8g\")"
      ],
      "metadata": {
        "id": "RJEeebdE5V2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please upload input file to sample_data folder before running!"
      ],
      "metadata": {
        "id": "4xAz2TdlzfCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload txt file under sample data\n",
        "txt = spark.read.text(\"/content/sample_data/soc-LiveJournal1Adj.txt\")"
      ],
      "metadata": {
        "id": "ZK19DTU1992r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add users to \"user\" column and friends to \"friends\" column\n",
        "txt = txt.withColumn(\"user\", split(txt.value, \"\\t\")[0])\n",
        "txt = txt.withColumn(\"friends\", split(txt.value, \"\\t\")[1])"
      ],
      "metadata": {
        "id": "HBQp6q_q-NKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split friends column to get a list of friends\n",
        "txt = txt.withColumn(\"friends\", split(txt.friends, \",\"))"
      ],
      "metadata": {
        "id": "h1msQ28h_EIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "txt = txt.select(txt.user,txt.friends)"
      ],
      "metadata": {
        "id": "NMWWrjXjAIF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#explode the data to get user , friend\n",
        "txt.select(txt.user,explode(txt.friends).alias(\"friend\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxcpROZ5AcVY",
        "outputId": "675eddcc-8e1f-42e5-e16c-fc6e866a7435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------+\n",
            "|user|friend|\n",
            "+----+------+\n",
            "|0   |1     |\n",
            "|0   |2     |\n",
            "|0   |3     |\n",
            "|0   |4     |\n",
            "|0   |5     |\n",
            "|0   |6     |\n",
            "|0   |7     |\n",
            "|0   |8     |\n",
            "|0   |9     |\n",
            "|0   |10    |\n",
            "|0   |11    |\n",
            "|0   |12    |\n",
            "|0   |13    |\n",
            "|0   |14    |\n",
            "|0   |15    |\n",
            "|0   |16    |\n",
            "|0   |17    |\n",
            "|0   |18    |\n",
            "|0   |19    |\n",
            "|0   |20    |\n",
            "+----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "friends = txt.select(txt.user,explode(txt.friends).alias(\"friend\"))"
      ],
      "metadata": {
        "id": "Z6RODiQKDgMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign big value to direct friends to be able to filter direct friends out of recommendation\n",
        "map_friend = friends.rdd.map(lambda row: ((row.user, row.friend),-9999999))\n",
        "#Take permutation of each list of friends to make a map of mutual friends\n",
        "map_mutual = txt.rdd.flatMap(lambda row: [(mutual,1) for mutual in itertools.permutations(row.friends, 2)])"
      ],
      "metadata": {
        "id": "TXr7CoSNDsVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source for flatMap: https://stackoverflow.com/questions/61053329/how-to-convert-rdd-list-of-lists-into-one-list-in-pyspark"
      ],
      "metadata": {
        "id": "C1VENGJW6rhz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce by key to get the no. of occurance of mutual friends\n",
        "map_mutual = map_mutual.reduceByKey(lambda a,b: a+b)"
      ],
      "metadata": {
        "id": "VIoEOkjbBLa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map_mutual.take(10)"
      ],
      "metadata": {
        "id": "aM5hR9wNCyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Combining the two maps\n",
        "map_mutual = map_mutual.union(map_friend)"
      ],
      "metadata": {
        "id": "vV1NDYhe7eYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source for union command: https://stackoverflow.com/questions/27395420/concatenating-datasets-of-different-rdds-in-apache-spark-using-scala"
      ],
      "metadata": {
        "id": "s3icwVC72w3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reduce by key to get \"count\" of direct friends go to -ve value.\n",
        "reduced = map_mutual.reduceByKey(lambda a,b: a+b)"
      ],
      "metadata": {
        "id": "lTG0REOw_JLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Filter out direct friends\n",
        "reduced = reduced.filter(lambda x: x[1] > 0)"
      ],
      "metadata": {
        "id": "1-ZxwDWgMzqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduced.take(10)"
      ],
      "metadata": {
        "id": "rmkc1jaXNVBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ((user,mutual),count): [(user,(count,mutual))]\n",
        "new_mutualFriends = reduced.map(lambda x: (x[0][0],(x[1],x[0][1])))"
      ],
      "metadata": {
        "id": "DXaFrs7dGyjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Group by key to get list of recommendation for each user.\n",
        "new_mutualFriends = new_mutualFriends.groupByKey().mapValues(list)"
      ],
      "metadata": {
        "id": "EI1gOqXx3v9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source for sorting list of tuples: https://stackoverflow.com/questions/34618029/how-to-sort-rdd-of-nested-list-structure-by-value-in-spark"
      ],
      "metadata": {
        "id": "ty_YNeWwDD7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Sort the count in descending order and the users with same count in ascending order.\n",
        "# (k, v): (k, sorted(v, key=lambda x: x[1], reverse=True))\n",
        "new_mutualFriends = new_mutualFriends.map(lambda x: (x[0], sorted(x[1], key=lambda y:(y[0], -int(y[1])), reverse=True)))"
      ],
      "metadata": {
        "id": "7T7j7Gyi_wan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Take the top 20 recommendation for each user\n",
        "n = 20\n",
        "new_mutualFriends = new_mutualFriends.map(lambda x:(x[0],x[1][:n]))"
      ],
      "metadata": {
        "id": "Cj2h82pqRCfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations = new_mutualFriends.map(lambda x: (x[0],[y[1] for y in x[1]]))"
      ],
      "metadata": {
        "id": "UJMZZpVxT-aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the recommendations as a list\n",
        "recommendationList = recommendations.collect()"
      ],
      "metadata": {
        "id": "tZRsxByKFeIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Find users with no friends or no mutual friends by comparing it with original file\n",
        "txt1 = spark.read.text(\"/content/sample_data/soc-LiveJournal1Adj.txt\")\n",
        "txt1 = txt1.withColumn(\"values\", split(txt1.value, \"\\t\")[0])\n",
        "empty = txt1.rdd.map(lambda row: (row.values,1))\n",
        "notEmpty = recommendations.map(lambda row: (row[0],1))\n",
        "fullList = empty.union(notEmpty)\n",
        "emptyList = fullList.reduceByKey(lambda a,b: a-b).filter(lambda x: x[1]>0).map(lambda x: (x[0],[])).collect()"
      ],
      "metadata": {
        "id": "xXizX7xGFnLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the empty list and recommendation list to get the final list. Also, sorting the list with users in ascending order.\n",
        "def getkey(x):\n",
        "  return int(x[0])\n",
        "finalRecommendation = recommendationList + emptyList\n",
        "finalRecommendation.sort(key= getkey)"
      ],
      "metadata": {
        "id": "exGZNJDXNopV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(finalRecommendation)"
      ],
      "metadata": {
        "id": "yxP2ao6_5kkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53bd9f94-8871-4953-93b4-bb96e1bc63fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "49995"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in finalRecommendation:\n",
        "  if int(x[0]) == 11 or int(x[0]) == 924 or int(x[0]) == 8941 or int(x[0]) == 8942 or int(x[0]) == 9019 or int(x[0]) == 9020 or int(x[0]) == 9021 or int(x[0]) == 9022 or int(x[0]) == 9990 or int(x[0]) == 9992 or int(x[0]) == 9993:\n",
        "    print (x[0],\"\\t\",','.join(x[1]))"
      ],
      "metadata": {
        "id": "ZDi_rzMnNrSx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fda540-65ee-4517-a2a3-386239d4e167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11 \t 27552,7785,27573,27574,27589,27590,27600,27617,27620,27667,32072,33192,10,12,110,638,1797,2141,5784,6893\n",
            "924 \t 439,2409,6995,11860,15416,43748,45881\n",
            "8941 \t 8943,8944,8940\n",
            "8942 \t 8939,8940,8943,8944\n",
            "9019 \t 9022,317,9023\n",
            "9020 \t 9021,9016,9017,9022,317,9023\n",
            "9021 \t 9020,9016,9017,9022,317,9023\n",
            "9022 \t 9019,9020,9021,317,9016,9017,9023\n",
            "9990 \t 13134,13478,13877,34299,34485,34642,37941\n",
            "9992 \t 9987,9989,35667,9991\n",
            "9993 \t 9991,13134,13478,13877,34299,34485,34642,37941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the output.txt file with all recommendations\n",
        "f = open('/content/sample_data/output.txt', 'w')\n",
        "for line in finalRecommendation:\n",
        "  f.write(\"\"+line[0]+\"\\t\"+','.join(line[1])+\"\\n\")"
      ],
      "metadata": {
        "id": "B5j-68HOSj5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}